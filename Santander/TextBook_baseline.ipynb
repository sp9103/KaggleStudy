{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:145: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:146: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.77959\n",
      "[1]\ttrain-mlogloss:2.48942\n",
      "[2]\ttrain-mlogloss:2.30662\n",
      "[3]\ttrain-mlogloss:2.16682\n",
      "[4]\ttrain-mlogloss:2.0523\n",
      "[5]\ttrain-mlogloss:1.95127\n",
      "[6]\ttrain-mlogloss:1.87573\n",
      "[7]\ttrain-mlogloss:1.80123\n",
      "[8]\ttrain-mlogloss:1.73604\n",
      "[9]\ttrain-mlogloss:1.68193\n",
      "[10]\ttrain-mlogloss:1.6308\n",
      "[11]\ttrain-mlogloss:1.58826\n",
      "[12]\ttrain-mlogloss:1.54855\n",
      "[13]\ttrain-mlogloss:1.5143\n",
      "[14]\ttrain-mlogloss:1.48184\n",
      "[15]\ttrain-mlogloss:1.45171\n",
      "[16]\ttrain-mlogloss:1.42467\n",
      "[17]\ttrain-mlogloss:1.40003\n",
      "[18]\ttrain-mlogloss:1.37783\n",
      "[19]\ttrain-mlogloss:1.35797\n",
      "[20]\ttrain-mlogloss:1.33823\n",
      "[21]\ttrain-mlogloss:1.32002\n",
      "[22]\ttrain-mlogloss:1.30312\n",
      "[23]\ttrain-mlogloss:1.28776\n",
      "[24]\ttrain-mlogloss:1.27389\n",
      "[25]\ttrain-mlogloss:1.26121\n",
      "[26]\ttrain-mlogloss:1.24913\n",
      "[27]\ttrain-mlogloss:1.23751\n",
      "[28]\ttrain-mlogloss:1.22743\n",
      "[29]\ttrain-mlogloss:1.21746\n",
      "[30]\ttrain-mlogloss:1.20883\n",
      "[31]\ttrain-mlogloss:1.20007\n",
      "[32]\ttrain-mlogloss:1.19204\n",
      "[33]\ttrain-mlogloss:1.18453\n",
      "[34]\ttrain-mlogloss:1.17718\n",
      "[35]\ttrain-mlogloss:1.17064\n",
      "[36]\ttrain-mlogloss:1.16429\n",
      "[37]\ttrain-mlogloss:1.15849\n",
      "[38]\ttrain-mlogloss:1.15293\n",
      "[39]\ttrain-mlogloss:1.14751\n",
      "[40]\ttrain-mlogloss:1.14262\n",
      "[41]\ttrain-mlogloss:1.13808\n",
      "[42]\ttrain-mlogloss:1.13355\n",
      "[43]\ttrain-mlogloss:1.12931\n",
      "[44]\ttrain-mlogloss:1.12536\n",
      "[45]\ttrain-mlogloss:1.12154\n",
      "[46]\ttrain-mlogloss:1.11803\n",
      "[47]\ttrain-mlogloss:1.11456\n",
      "[48]\ttrain-mlogloss:1.11148\n",
      "[49]\ttrain-mlogloss:1.10828\n",
      "[50]\ttrain-mlogloss:1.10528\n",
      "[51]\ttrain-mlogloss:1.10235\n",
      "[52]\ttrain-mlogloss:1.09963\n",
      "[53]\ttrain-mlogloss:1.09724\n",
      "[54]\ttrain-mlogloss:1.09465\n",
      "[55]\ttrain-mlogloss:1.09227\n",
      "[56]\ttrain-mlogloss:1.09013\n",
      "[57]\ttrain-mlogloss:1.08803\n",
      "[58]\ttrain-mlogloss:1.08601\n",
      "[59]\ttrain-mlogloss:1.08408\n",
      "[60]\ttrain-mlogloss:1.08233\n",
      "[61]\ttrain-mlogloss:1.08053\n",
      "[62]\ttrain-mlogloss:1.07886\n",
      "[63]\ttrain-mlogloss:1.0772\n",
      "[64]\ttrain-mlogloss:1.07561\n",
      "[65]\ttrain-mlogloss:1.07393\n",
      "[66]\ttrain-mlogloss:1.07244\n",
      "[67]\ttrain-mlogloss:1.07088\n",
      "[68]\ttrain-mlogloss:1.06958\n",
      "[69]\ttrain-mlogloss:1.06826\n",
      "[70]\ttrain-mlogloss:1.06699\n",
      "[71]\ttrain-mlogloss:1.0658\n",
      "[72]\ttrain-mlogloss:1.06459\n",
      "[73]\ttrain-mlogloss:1.06347\n",
      "[74]\ttrain-mlogloss:1.06228\n",
      "[75]\ttrain-mlogloss:1.06121\n",
      "[76]\ttrain-mlogloss:1.06018\n",
      "[77]\ttrain-mlogloss:1.05912\n",
      "[78]\ttrain-mlogloss:1.0581\n",
      "[79]\ttrain-mlogloss:1.05722\n",
      "[80]\ttrain-mlogloss:1.05627\n",
      "[81]\ttrain-mlogloss:1.05532\n",
      "[82]\ttrain-mlogloss:1.05439\n",
      "[83]\ttrain-mlogloss:1.05349\n",
      "[84]\ttrain-mlogloss:1.05257\n",
      "[85]\ttrain-mlogloss:1.05171\n",
      "[86]\ttrain-mlogloss:1.05078\n",
      "[87]\ttrain-mlogloss:1.04999\n",
      "[88]\ttrain-mlogloss:1.04926\n",
      "[89]\ttrain-mlogloss:1.04852\n",
      "[90]\ttrain-mlogloss:1.04775\n",
      "[91]\ttrain-mlogloss:1.04703\n",
      "[92]\ttrain-mlogloss:1.04635\n",
      "[93]\ttrain-mlogloss:1.0456\n",
      "[94]\ttrain-mlogloss:1.04479\n",
      "[95]\ttrain-mlogloss:1.04395\n",
      "[96]\ttrain-mlogloss:1.04328\n",
      "[97]\ttrain-mlogloss:1.0426\n",
      "[98]\ttrain-mlogloss:1.04192\n",
      "[99]\ttrain-mlogloss:1.0413\n",
      "[100]\ttrain-mlogloss:1.04075\n",
      "[101]\ttrain-mlogloss:1.04011\n",
      "[102]\ttrain-mlogloss:1.03951\n",
      "[103]\ttrain-mlogloss:1.03899\n",
      "[104]\ttrain-mlogloss:1.03833\n",
      "[105]\ttrain-mlogloss:1.03777\n",
      "[106]\ttrain-mlogloss:1.0372\n",
      "[107]\ttrain-mlogloss:1.0365\n",
      "[108]\ttrain-mlogloss:1.03596\n",
      "[109]\ttrain-mlogloss:1.03536\n",
      "[110]\ttrain-mlogloss:1.03482\n",
      "[111]\ttrain-mlogloss:1.03408\n",
      "[112]\ttrain-mlogloss:1.03358\n",
      "[113]\ttrain-mlogloss:1.03308\n",
      "[114]\ttrain-mlogloss:1.03257\n",
      "[115]\ttrain-mlogloss:1.032\n",
      "[116]\ttrain-mlogloss:1.03148\n",
      "[117]\ttrain-mlogloss:1.03098\n",
      "[118]\ttrain-mlogloss:1.0304\n",
      "[119]\ttrain-mlogloss:1.02982\n",
      "[120]\ttrain-mlogloss:1.02919\n",
      "[121]\ttrain-mlogloss:1.02874\n",
      "[122]\ttrain-mlogloss:1.02824\n",
      "[123]\ttrain-mlogloss:1.02775\n",
      "[124]\ttrain-mlogloss:1.02729\n",
      "[125]\ttrain-mlogloss:1.02671\n",
      "[126]\ttrain-mlogloss:1.02619\n",
      "[127]\ttrain-mlogloss:1.02574\n",
      "[128]\ttrain-mlogloss:1.02517\n",
      "[129]\ttrain-mlogloss:1.0247\n",
      "[130]\ttrain-mlogloss:1.02424\n",
      "[131]\ttrain-mlogloss:1.02366\n",
      "[132]\ttrain-mlogloss:1.02319\n",
      "[133]\ttrain-mlogloss:1.0228\n",
      "[134]\ttrain-mlogloss:1.02222\n",
      "[135]\ttrain-mlogloss:1.02173\n",
      "[136]\ttrain-mlogloss:1.02125\n",
      "[137]\ttrain-mlogloss:1.02075\n",
      "[138]\ttrain-mlogloss:1.0202\n",
      "[139]\ttrain-mlogloss:1.01971\n",
      "[140]\ttrain-mlogloss:1.01936\n",
      "[141]\ttrain-mlogloss:1.0188\n",
      "[142]\ttrain-mlogloss:1.01828\n",
      "[143]\ttrain-mlogloss:1.0177\n",
      "[144]\ttrain-mlogloss:1.01732\n",
      "[145]\ttrain-mlogloss:1.01687\n",
      "[146]\ttrain-mlogloss:1.01643\n",
      "[147]\ttrain-mlogloss:1.01585\n",
      "[148]\ttrain-mlogloss:1.01549\n",
      "[149]\ttrain-mlogloss:1.01501\n",
      "[150]\ttrain-mlogloss:1.01447\n",
      "[151]\ttrain-mlogloss:1.01401\n",
      "[152]\ttrain-mlogloss:1.01346\n",
      "[153]\ttrain-mlogloss:1.01304\n",
      "[154]\ttrain-mlogloss:1.01247\n",
      "[155]\ttrain-mlogloss:1.01201\n",
      "[156]\ttrain-mlogloss:1.01157\n",
      "[157]\ttrain-mlogloss:1.0111\n",
      "[158]\ttrain-mlogloss:1.01062\n",
      "[159]\ttrain-mlogloss:1.01013\n",
      "[160]\ttrain-mlogloss:1.00972\n",
      "[161]\ttrain-mlogloss:1.00916\n",
      "[162]\ttrain-mlogloss:1.00869\n",
      "[163]\ttrain-mlogloss:1.0083\n",
      "[164]\ttrain-mlogloss:1.00785\n",
      "[165]\ttrain-mlogloss:1.00732\n",
      "[166]\ttrain-mlogloss:1.00683\n",
      "[167]\ttrain-mlogloss:1.00628\n",
      "[168]\ttrain-mlogloss:1.00576\n",
      "[169]\ttrain-mlogloss:1.0052\n",
      "[170]\ttrain-mlogloss:1.00468\n",
      "[171]\ttrain-mlogloss:1.00417\n",
      "[172]\ttrain-mlogloss:1.00369\n",
      "[173]\ttrain-mlogloss:1.00325\n",
      "[174]\ttrain-mlogloss:1.0026\n",
      "[175]\ttrain-mlogloss:1.00219\n",
      "[176]\ttrain-mlogloss:1.00161\n",
      "[177]\ttrain-mlogloss:1.00107\n",
      "[178]\ttrain-mlogloss:1.00062\n",
      "[179]\ttrain-mlogloss:1.00016\n",
      "[180]\ttrain-mlogloss:0.999756\n",
      "[181]\ttrain-mlogloss:0.999176\n",
      "[182]\ttrain-mlogloss:0.99877\n",
      "[183]\ttrain-mlogloss:0.998327\n",
      "[184]\ttrain-mlogloss:0.997774\n",
      "[185]\ttrain-mlogloss:0.997256\n",
      "[186]\ttrain-mlogloss:0.996815\n",
      "[187]\ttrain-mlogloss:0.996354\n",
      "[188]\ttrain-mlogloss:0.9959\n",
      "[189]\ttrain-mlogloss:0.995356\n",
      "[190]\ttrain-mlogloss:0.994876\n",
      "[191]\ttrain-mlogloss:0.994342\n",
      "[192]\ttrain-mlogloss:0.993878\n",
      "[193]\ttrain-mlogloss:0.993453\n",
      "[194]\ttrain-mlogloss:0.992976\n",
      "[195]\ttrain-mlogloss:0.992566\n",
      "[196]\ttrain-mlogloss:0.992175\n",
      "[197]\ttrain-mlogloss:0.99173\n",
      "[198]\ttrain-mlogloss:0.991352\n",
      "[199]\ttrain-mlogloss:0.990876\n",
      "[200]\ttrain-mlogloss:0.990343\n",
      "[201]\ttrain-mlogloss:0.98991\n",
      "[202]\ttrain-mlogloss:0.98946\n",
      "[203]\ttrain-mlogloss:0.98901\n",
      "[204]\ttrain-mlogloss:0.988585\n",
      "[205]\ttrain-mlogloss:0.988117\n",
      "[206]\ttrain-mlogloss:0.987546\n",
      "[207]\ttrain-mlogloss:0.987127\n",
      "[208]\ttrain-mlogloss:0.98668\n",
      "[209]\ttrain-mlogloss:0.98622\n",
      "[210]\ttrain-mlogloss:0.98581\n",
      "[211]\ttrain-mlogloss:0.985526\n",
      "[212]\ttrain-mlogloss:0.985063\n",
      "[213]\ttrain-mlogloss:0.984663\n",
      "[214]\ttrain-mlogloss:0.984167\n",
      "[215]\ttrain-mlogloss:0.983674\n",
      "[216]\ttrain-mlogloss:0.983262\n",
      "[217]\ttrain-mlogloss:0.982765\n",
      "[218]\ttrain-mlogloss:0.982206\n",
      "[219]\ttrain-mlogloss:0.981822\n",
      "[220]\ttrain-mlogloss:0.981279\n",
      "[221]\ttrain-mlogloss:0.980814\n",
      "[222]\ttrain-mlogloss:0.980426\n",
      "[223]\ttrain-mlogloss:0.979866\n",
      "[224]\ttrain-mlogloss:0.979399\n",
      "[225]\ttrain-mlogloss:0.979058\n",
      "[226]\ttrain-mlogloss:0.978678\n",
      "[227]\ttrain-mlogloss:0.978115\n",
      "[228]\ttrain-mlogloss:0.977508\n",
      "[229]\ttrain-mlogloss:0.977064\n",
      "[230]\ttrain-mlogloss:0.976563\n",
      "[231]\ttrain-mlogloss:0.976141\n",
      "[232]\ttrain-mlogloss:0.975645\n",
      "[233]\ttrain-mlogloss:0.975203\n",
      "[234]\ttrain-mlogloss:0.974839\n",
      "[235]\ttrain-mlogloss:0.974369\n",
      "[236]\ttrain-mlogloss:0.973988\n",
      "[237]\ttrain-mlogloss:0.973513\n",
      "[238]\ttrain-mlogloss:0.973071\n",
      "[239]\ttrain-mlogloss:0.97261\n",
      "[240]\ttrain-mlogloss:0.972187\n",
      "[241]\ttrain-mlogloss:0.971778\n",
      "[242]\ttrain-mlogloss:0.971209\n",
      "[243]\ttrain-mlogloss:0.970746\n",
      "[244]\ttrain-mlogloss:0.970275\n",
      "[245]\ttrain-mlogloss:0.969876\n",
      "[246]\ttrain-mlogloss:0.969495\n",
      "[247]\ttrain-mlogloss:0.969023\n",
      "[248]\ttrain-mlogloss:0.96854\n",
      "[249]\ttrain-mlogloss:0.968134\n",
      "[250]\ttrain-mlogloss:0.967687\n",
      "[251]\ttrain-mlogloss:0.967082\n",
      "[252]\ttrain-mlogloss:0.96674\n",
      "[253]\ttrain-mlogloss:0.966202\n",
      "[254]\ttrain-mlogloss:0.965718\n",
      "[255]\ttrain-mlogloss:0.965272\n",
      "[256]\ttrain-mlogloss:0.964934\n",
      "[257]\ttrain-mlogloss:0.964582\n",
      "[258]\ttrain-mlogloss:0.964172\n",
      "[259]\ttrain-mlogloss:0.963797\n",
      "[260]\ttrain-mlogloss:0.963461\n",
      "[261]\ttrain-mlogloss:0.963016\n",
      "[262]\ttrain-mlogloss:0.962598\n",
      "[263]\ttrain-mlogloss:0.962227\n",
      "[264]\ttrain-mlogloss:0.961851\n",
      "[265]\ttrain-mlogloss:0.961559\n",
      "[266]\ttrain-mlogloss:0.961175\n",
      "[267]\ttrain-mlogloss:0.960677\n",
      "[268]\ttrain-mlogloss:0.960338\n",
      "[269]\ttrain-mlogloss:0.959943\n",
      "[270]\ttrain-mlogloss:0.959509\n",
      "[271]\ttrain-mlogloss:0.95914\n",
      "[272]\ttrain-mlogloss:0.958786\n",
      "[273]\ttrain-mlogloss:0.958488\n",
      "[274]\ttrain-mlogloss:0.958098\n",
      "[275]\ttrain-mlogloss:0.957643\n",
      "[276]\ttrain-mlogloss:0.957299\n",
      "[277]\ttrain-mlogloss:0.956897\n",
      "[278]\ttrain-mlogloss:0.956505\n",
      "[279]\ttrain-mlogloss:0.956114\n",
      "[280]\ttrain-mlogloss:0.955709\n",
      "[281]\ttrain-mlogloss:0.955408\n",
      "[282]\ttrain-mlogloss:0.954993\n",
      "[283]\ttrain-mlogloss:0.95466\n",
      "[284]\ttrain-mlogloss:0.95409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[285]\ttrain-mlogloss:0.953616\n",
      "[286]\ttrain-mlogloss:0.953174\n",
      "[287]\ttrain-mlogloss:0.952772\n",
      "[288]\ttrain-mlogloss:0.952379\n",
      "[289]\ttrain-mlogloss:0.951791\n",
      "[290]\ttrain-mlogloss:0.951335\n",
      "[291]\ttrain-mlogloss:0.950956\n",
      "[292]\ttrain-mlogloss:0.950557\n",
      "[293]\ttrain-mlogloss:0.950119\n",
      "[294]\ttrain-mlogloss:0.949707\n",
      "[295]\ttrain-mlogloss:0.949244\n",
      "[296]\ttrain-mlogloss:0.948932\n",
      "[297]\ttrain-mlogloss:0.948635\n",
      "[298]\ttrain-mlogloss:0.948275\n",
      "[299]\ttrain-mlogloss:0.947801\n",
      "[300]\ttrain-mlogloss:0.947403\n",
      "[301]\ttrain-mlogloss:0.947093\n",
      "[302]\ttrain-mlogloss:0.946727\n",
      "[303]\ttrain-mlogloss:0.946343\n",
      "[304]\ttrain-mlogloss:0.945998\n",
      "[305]\ttrain-mlogloss:0.945574\n",
      "[306]\ttrain-mlogloss:0.945274\n",
      "[307]\ttrain-mlogloss:0.944923\n",
      "[308]\ttrain-mlogloss:0.944536\n",
      "[309]\ttrain-mlogloss:0.944216\n",
      "[310]\ttrain-mlogloss:0.943957\n",
      "[311]\ttrain-mlogloss:0.943653\n",
      "Feature importance:\n",
      "('renta', 27043)\n",
      "('age', 23947)\n",
      "('antiguedad', 23138)\n",
      "('age_prev', 16463)\n",
      "('antiguedad_prev', 16266)\n",
      "('fecha_alta_month', 15235)\n",
      "('nomprov', 14467)\n",
      "('fecha_alta_year', 11489)\n",
      "('renta_prev', 11277)\n",
      "('canal_entrada', 9670)\n",
      "('nomprov_prev', 8152)\n",
      "('canal_entrada_prev', 5952)\n",
      "('fecha_alta_month_prev', 5475)\n",
      "('sexo', 4223)\n",
      "('fecha_alta_year_prev', 3973)\n",
      "('ind_recibo_ult1_prev', 3960)\n",
      "('ind_ecue_fin_ult1_prev', 3562)\n",
      "('ind_cco_fin_ult1_prev', 3525)\n",
      "('ind_cno_fin_ult1_prev', 3379)\n",
      "('segmento', 2645)\n",
      "('ind_reca_fin_ult1_prev', 2539)\n",
      "('ind_tjcr_fin_ult1_prev', 2446)\n",
      "('segmento_prev', 2329)\n",
      "('tiprel_1mes', 1860)\n",
      "('ind_valo_fin_ult1_prev', 1823)\n",
      "('ind_nom_pens_ult1_prev', 1814)\n",
      "('ind_ctop_fin_ult1_prev', 1799)\n",
      "('ind_dela_fin_ult1_prev', 1771)\n",
      "('ind_nomina_ult1_prev', 1653)\n",
      "('sexo_prev', 1512)\n",
      "('ind_ctpp_fin_ult1_prev', 1451)\n",
      "('ind_actividad_cliente', 1406)\n",
      "('tiprel_1mes_prev', 1345)\n",
      "('ind_fond_fin_ult1_prev', 1169)\n",
      "('ind_actividad_cliente_prev', 1055)\n",
      "('ind_ctma_fin_ult1_prev', 982)\n",
      "('indext', 941)\n",
      "('ind_nuevo', 803)\n",
      "('ind_plan_fin_ult1_prev', 753)\n",
      "('ind_hip_fin_ult1_prev', 640)\n",
      "('ind_nuevo_prev', 471)\n",
      "('indrel_1mes', 450)\n",
      "('indext_prev', 435)\n",
      "('ind_deco_fin_ult1_prev', 404)\n",
      "('pais_residencia', 302)\n",
      "('indrel_1mes_prev', 242)\n",
      "('ind_viv_fin_ult1_prev', 216)\n",
      "('ind_empleado_prev', 215)\n",
      "('pais_residencia_prev', 202)\n",
      "('ind_empleado', 197)\n",
      "('indrel', 191)\n",
      "('ind_deme_fin_ult1_prev', 138)\n",
      "('ind_pres_fin_ult1_prev', 115)\n",
      "('ind_ctju_fin_ult1_prev', 77)\n",
      "('ult_fec_cli_1t_month', 68)\n",
      "('ind_cder_fin_ult1_prev', 61)\n",
      "('indfall_prev', 37)\n",
      "('indfall', 28)\n",
      "('conyuemp_prev', 26)\n",
      "('indresi', 18)\n",
      "('ult_fec_cli_1t_year', 18)\n",
      "('indresi_prev', 9)\n",
      "('conyuemp', 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:160: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:163: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:164: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from mapk import *\n",
    "\n",
    "np.random.seed(2018)\n",
    "\n",
    "# 데이터를 불러온다.\n",
    "trn = pd.read_csv('./dataset/train_ver2.csv')\n",
    "tst = pd.read_csv('./dataset/test_ver2.csv')\n",
    "\n",
    "## 데이터 전처리 ##\n",
    "\n",
    "# 제품 변수를 별도로 저장해 놓는다.\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "# 제품 변수 결측값을 미리 0으로 대체한다.\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거한다.\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 통합한다. 테스트 데이터에 없는 제품 변수는 0으로 채운다.\n",
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "df = pd.concat([trn, tst], axis=0)\n",
    "\n",
    "# 학습에 사용할 변수를 담는 list이다.\n",
    "features = []\n",
    "\n",
    "# 범주형 변수를 .factorize() 함수를 통해 label encoding한다.\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols\n",
    "\n",
    "# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환한다.\n",
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)\n",
    "\n",
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)\n",
    "\n",
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)\n",
    "\n",
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n",
    "\n",
    "# 학습에 사용할 수치형 변수를 features에 추구한다.\n",
    "features += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']\n",
    "\n",
    "# (피쳐 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출한다.\n",
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']\n",
    "\n",
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n",
    "\n",
    "# 그 외 변수의 결측값은 모두 -99로 대체한다.\n",
    "df.fillna(-99, inplace=True)\n",
    "\n",
    "# (피쳐 엔지니어링) lag-1 데이터를 생성한다.\n",
    "# 코드 2-12와 유사한 코드 흐름이다.\n",
    "\n",
    "# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\n",
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    return int_date\n",
    "\n",
    "# 날짜를 숫자로 변환하여 int_date에 저장한다\n",
    "df['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n",
    "\n",
    "# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\n",
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\n",
    "df_lag['int_date'] += 1\n",
    "\n",
    "# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려 있기 때문에, 저번 달의 제품 정보가 삽입된다.\n",
    "df_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n",
    "\n",
    "# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\n",
    "del df, df_lag\n",
    "\n",
    "# 저번 달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)\n",
    "\n",
    "# lag-1 변수를 추가한다.\n",
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]\n",
    "\n",
    "###\n",
    "### Baseline 모델 이후, 다양한 피쳐 엔지니어링을 여기에 추가한다.\n",
    "###\n",
    "\n",
    "\n",
    "## 모델 학습\n",
    "# 학습을 위하여 데이터를 훈련, 테스트용으로 분리한다.\n",
    "# 학습에는 2016-01-28 ~ 2016-04-28 데이터만 사용하고, 검증에는 2016-05-28 데이터를 사용한다.\n",
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn\n",
    "\n",
    "# 훈련 데이터에서 신규 구매 건수만 추출한다.\n",
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y\n",
    "\n",
    "\n",
    "# XGBoost 모델 parameter를 설정한다.\n",
    "param = {\n",
    "    'booster': 'gbtree',\n",
    "    'max_depth': 8,\n",
    "    'nthread': 8,\n",
    "    'num_class': len(prods),\n",
    "    'objective': 'multi:softprob',\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'eta': 0.1,\n",
    "    'min_child_weight': 10,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'colsample_bylevel': 0.9,\n",
    "    'seed': 2018,\n",
    "    }\n",
    "\n",
    "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\n",
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]\n",
    "# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가한다.\n",
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "# XGBoost 모델 재학습!\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n",
    "\n",
    "# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\n",
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)\n",
    "\n",
    "# 캐글 제출을 위하여 테스트 데이터에 대한 예측 값을 구한다.\n",
    "X_tst = tst.as_matrix(columns=features)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\n",
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])\n",
    "\n",
    "# 제출 파일을 생성한다.\n",
    "submit_file = open('./model/xgb.baseline.2015-06-28', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일을 생성한다.\n",
    "count = 0\n",
    "submit_file = open('./model/xgb.baseline.2019-06-17', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "#     print('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n",
    "#     count += 1\n",
    "#     if count > 100:\n",
    "#         break\n",
    "        \n",
    "submit_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ncodpers_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
