{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (5,8,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "trn = pd.read_csv('./dataset/train_ver2.csv')\n",
    "tst = pd.read_csv('./dataset/test_ver2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tranin set shape :  (13647309, 48)\n",
      "after tranin set shape :  (11091070, 48)\n"
     ]
    }
   ],
   "source": [
    "print('before tranin set shape : ', trn.shape)\n",
    "prods = trn.columns[24:].tolist()\n",
    "\n",
    "trn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n",
    "\n",
    "# if some customer buy anyting, remove\n",
    "no_product = trn[prods].sum(axis=1) == 0\n",
    "trn = trn[~no_product]\n",
    "print('after tranin set shape : ', trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in trn.columns[24:]:\n",
    "    tst[col] = 0\n",
    "\n",
    "df = pd.concat([trn, tst], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "# 범주형을 0, 1, 2와 같은 꼴로 변경시킴\n",
    "categorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'nomprov', 'segmento']\n",
    "for col in categorical_cols:\n",
    "    df[col], _ = df[col].factorize(na_sentinel=-99)\n",
    "features += categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].replace(' NA', -99, inplace=True)\n",
    "df['age'] = df['age'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['antiguedad'].replace('     NA', -99, inplace=True)\n",
    "df['antiguedad'] = df['antiguedad'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['renta'].replace('         NA', -99, inplace=True)\n",
    "df['renta'].fillna(-99, inplace=True)\n",
    "df['renta'] = df['renta'].astype(float).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indrel_1mes'].replace('P', 5, inplace=True)\n",
    "df['indrel_1mes'].fillna(-99, inplace=True)\n",
    "df['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += ['age', 'antiguedad', 'renta', 'ind_nuevo', 'indrel', 'intrel_1mes', 'ind_actividad_cliente']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['tipodom']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['fecha_alta_month', 'fecha_alta_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\n",
    "df['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\n",
    "features += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ind_empleado',\n",
       " 'pais_residencia',\n",
       " 'sexo',\n",
       " 'tiprel_1mes',\n",
       " 'indresi',\n",
       " 'indext',\n",
       " 'conyuemp',\n",
       " 'canal_entrada',\n",
       " 'indfall',\n",
       " 'nomprov',\n",
       " 'segmento',\n",
       " 'age',\n",
       " 'antiguedad',\n",
       " 'renta',\n",
       " 'ind_nuevo',\n",
       " 'indrel',\n",
       " 'intrel_1mes',\n",
       " 'ind_actividad_cliente',\n",
       " 'fecha_alta_month',\n",
       " 'fecha_alta_year',\n",
       " 'ult_fec_cli_1t_month',\n",
       " 'ult_fec_cli_1t_year']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_int(str_date):\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    \n",
    "    return int_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_date'] = trn['fecha_dato'].map(date_to_int).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = df.copy()\n",
    "df_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns]\n",
    "df_lag['int_date'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print(len(df_lag.columns))\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = df.merge(df_lag, on=['ncodpers', 'int_date'], how='left')\n",
    "del df, df_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    df_trn[prev].fillna(0, inplace=True)\n",
    "df_trn.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features += [feature + '_prev' for feature in features]\n",
    "features += [prod + '_prev' for prod in prods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\n",
    "trn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\n",
    "tst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\n",
    "del df_trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for i, prod in enumerate(prods):\n",
    "    prev = prod + '_prev'\n",
    "    #Extract new purchase\n",
    "    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n",
    "    prY = np.zeros(prX.shape[0], dtype = np.int8) + i\n",
    "    X.append(prX)\n",
    "    Y.append(prY)\n",
    "XY = pd.concat(X)\n",
    "Y = np.hstack(Y)\n",
    "XY['y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vld_date = '2016-05-28'\n",
    "XY_trn = XY[XY['fecha_dato'] != vld_date]\n",
    "XY_vld = XY[XY['fecha_dato'] == vld_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'booster' : 'gbtree',\n",
    "    'max_depth' : 8,\n",
    "    'nthread' : 4,\n",
    "    'num_class' : len(prods),\n",
    "    'objective' : 'multi:softprob',\n",
    "    'silent' : 1,\n",
    "    'eval_metric' : 'mlogloss',\n",
    "    'eta' : 0.1,\n",
    "    'min_child_weight' : 10,\n",
    "    'colsample_bytree' : 0.8,\n",
    "    'colsample_bylevel' : 0.9,\n",
    "    'seed': 2019,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_trn = XY_trn.as_matrix(columns=features)\n",
    "Y_trn = XY_trn.as_matrix(columns=['y'])\n",
    "dtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_vld = XY_vld.as_matrix(columns=features)\n",
    "Y_vld = XY_vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.74803\teval-mlogloss:2.7587\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.5136\teval-mlogloss:2.53046\n",
      "[2]\ttrain-mlogloss:2.32368\teval-mlogloss:2.34079\n",
      "[3]\ttrain-mlogloss:2.17125\teval-mlogloss:2.18883\n",
      "[4]\ttrain-mlogloss:2.05224\teval-mlogloss:2.07009\n",
      "[5]\ttrain-mlogloss:1.9533\teval-mlogloss:1.97154\n",
      "[6]\ttrain-mlogloss:1.86737\teval-mlogloss:1.88576\n",
      "[7]\ttrain-mlogloss:1.79415\teval-mlogloss:1.81287\n",
      "[8]\ttrain-mlogloss:1.73029\teval-mlogloss:1.74942\n",
      "[9]\ttrain-mlogloss:1.67555\teval-mlogloss:1.69534\n",
      "[10]\ttrain-mlogloss:1.63034\teval-mlogloss:1.65059\n",
      "[11]\ttrain-mlogloss:1.58562\teval-mlogloss:1.60597\n",
      "[12]\ttrain-mlogloss:1.54695\teval-mlogloss:1.5678\n",
      "[13]\ttrain-mlogloss:1.51234\teval-mlogloss:1.5333\n",
      "[14]\ttrain-mlogloss:1.47955\teval-mlogloss:1.50079\n",
      "[15]\ttrain-mlogloss:1.44957\teval-mlogloss:1.47118\n",
      "[16]\ttrain-mlogloss:1.42302\teval-mlogloss:1.44483\n",
      "[17]\ttrain-mlogloss:1.39777\teval-mlogloss:1.41976\n",
      "[18]\ttrain-mlogloss:1.37586\teval-mlogloss:1.3982\n",
      "[19]\ttrain-mlogloss:1.35454\teval-mlogloss:1.37711\n",
      "[20]\ttrain-mlogloss:1.33561\teval-mlogloss:1.35868\n",
      "[21]\ttrain-mlogloss:1.31881\teval-mlogloss:1.34229\n",
      "[22]\ttrain-mlogloss:1.30225\teval-mlogloss:1.32606\n",
      "[23]\ttrain-mlogloss:1.28762\teval-mlogloss:1.3118\n",
      "[24]\ttrain-mlogloss:1.27374\teval-mlogloss:1.29835\n",
      "[25]\ttrain-mlogloss:1.26156\teval-mlogloss:1.2864\n",
      "[26]\ttrain-mlogloss:1.24913\teval-mlogloss:1.2743\n",
      "[27]\ttrain-mlogloss:1.238\teval-mlogloss:1.26347\n",
      "[28]\ttrain-mlogloss:1.22765\teval-mlogloss:1.25354\n",
      "[29]\ttrain-mlogloss:1.21776\teval-mlogloss:1.24395\n",
      "[30]\ttrain-mlogloss:1.20857\teval-mlogloss:1.23514\n",
      "[31]\ttrain-mlogloss:1.19999\teval-mlogloss:1.22685\n",
      "[32]\ttrain-mlogloss:1.19229\teval-mlogloss:1.21973\n",
      "[33]\ttrain-mlogloss:1.18452\teval-mlogloss:1.21237\n",
      "[34]\ttrain-mlogloss:1.1777\teval-mlogloss:1.20581\n",
      "[35]\ttrain-mlogloss:1.17088\teval-mlogloss:1.19944\n",
      "[36]\ttrain-mlogloss:1.16475\teval-mlogloss:1.19385\n",
      "[37]\ttrain-mlogloss:1.1589\teval-mlogloss:1.18845\n",
      "[38]\ttrain-mlogloss:1.15308\teval-mlogloss:1.18303\n",
      "[39]\ttrain-mlogloss:1.14777\teval-mlogloss:1.17811\n",
      "[40]\ttrain-mlogloss:1.14279\teval-mlogloss:1.17359\n",
      "[41]\ttrain-mlogloss:1.13815\teval-mlogloss:1.16941\n",
      "[42]\ttrain-mlogloss:1.13358\teval-mlogloss:1.16529\n",
      "[43]\ttrain-mlogloss:1.1293\teval-mlogloss:1.16147\n",
      "[44]\ttrain-mlogloss:1.12532\teval-mlogloss:1.158\n",
      "[45]\ttrain-mlogloss:1.12138\teval-mlogloss:1.15446\n",
      "[46]\ttrain-mlogloss:1.11776\teval-mlogloss:1.15125\n",
      "[47]\ttrain-mlogloss:1.11418\teval-mlogloss:1.14803\n",
      "[48]\ttrain-mlogloss:1.11106\teval-mlogloss:1.14533\n",
      "[49]\ttrain-mlogloss:1.10787\teval-mlogloss:1.14268\n",
      "[50]\ttrain-mlogloss:1.10474\teval-mlogloss:1.14002\n",
      "[51]\ttrain-mlogloss:1.10184\teval-mlogloss:1.13759\n",
      "[52]\ttrain-mlogloss:1.09902\teval-mlogloss:1.13515\n",
      "[53]\ttrain-mlogloss:1.09639\teval-mlogloss:1.13286\n",
      "[54]\ttrain-mlogloss:1.09391\teval-mlogloss:1.13077\n",
      "[55]\ttrain-mlogloss:1.09155\teval-mlogloss:1.12871\n",
      "[56]\ttrain-mlogloss:1.08929\teval-mlogloss:1.12681\n",
      "[57]\ttrain-mlogloss:1.08702\teval-mlogloss:1.12501\n",
      "[58]\ttrain-mlogloss:1.08485\teval-mlogloss:1.12327\n",
      "[59]\ttrain-mlogloss:1.08285\teval-mlogloss:1.12168\n",
      "[60]\ttrain-mlogloss:1.08093\teval-mlogloss:1.12012\n",
      "[61]\ttrain-mlogloss:1.07911\teval-mlogloss:1.11859\n",
      "[62]\ttrain-mlogloss:1.07744\teval-mlogloss:1.11729\n",
      "[63]\ttrain-mlogloss:1.07576\teval-mlogloss:1.116\n",
      "[64]\ttrain-mlogloss:1.07423\teval-mlogloss:1.11483\n",
      "[65]\ttrain-mlogloss:1.07273\teval-mlogloss:1.11368\n",
      "[66]\ttrain-mlogloss:1.07105\teval-mlogloss:1.11249\n",
      "[67]\ttrain-mlogloss:1.06952\teval-mlogloss:1.11134\n",
      "[68]\ttrain-mlogloss:1.06812\teval-mlogloss:1.11023\n",
      "[69]\ttrain-mlogloss:1.06672\teval-mlogloss:1.10928\n",
      "[70]\ttrain-mlogloss:1.06533\teval-mlogloss:1.10835\n",
      "[71]\ttrain-mlogloss:1.06404\teval-mlogloss:1.10756\n",
      "[72]\ttrain-mlogloss:1.06261\teval-mlogloss:1.10665\n",
      "[73]\ttrain-mlogloss:1.06138\teval-mlogloss:1.10585\n",
      "[74]\ttrain-mlogloss:1.06031\teval-mlogloss:1.1051\n",
      "[75]\ttrain-mlogloss:1.05921\teval-mlogloss:1.10443\n",
      "[76]\ttrain-mlogloss:1.05807\teval-mlogloss:1.10377\n",
      "[77]\ttrain-mlogloss:1.05702\teval-mlogloss:1.10311\n",
      "[78]\ttrain-mlogloss:1.05594\teval-mlogloss:1.10252\n",
      "[79]\ttrain-mlogloss:1.0548\teval-mlogloss:1.10191\n",
      "[80]\ttrain-mlogloss:1.05386\teval-mlogloss:1.10132\n",
      "[81]\ttrain-mlogloss:1.05283\teval-mlogloss:1.10074\n",
      "[82]\ttrain-mlogloss:1.05198\teval-mlogloss:1.10018\n",
      "[83]\ttrain-mlogloss:1.05092\teval-mlogloss:1.0997\n",
      "[84]\ttrain-mlogloss:1.05001\teval-mlogloss:1.09928\n",
      "[85]\ttrain-mlogloss:1.04915\teval-mlogloss:1.09875\n",
      "[86]\ttrain-mlogloss:1.04824\teval-mlogloss:1.09838\n",
      "[87]\ttrain-mlogloss:1.0473\teval-mlogloss:1.09803\n",
      "[88]\ttrain-mlogloss:1.0465\teval-mlogloss:1.09761\n",
      "[89]\ttrain-mlogloss:1.04566\teval-mlogloss:1.0972\n",
      "[90]\ttrain-mlogloss:1.04489\teval-mlogloss:1.09683\n",
      "[91]\ttrain-mlogloss:1.04403\teval-mlogloss:1.09647\n",
      "[92]\ttrain-mlogloss:1.04328\teval-mlogloss:1.09618\n",
      "[93]\ttrain-mlogloss:1.04251\teval-mlogloss:1.09587\n",
      "[94]\ttrain-mlogloss:1.04182\teval-mlogloss:1.09556\n",
      "[95]\ttrain-mlogloss:1.04111\teval-mlogloss:1.09529\n",
      "[96]\ttrain-mlogloss:1.04046\teval-mlogloss:1.09501\n",
      "[97]\ttrain-mlogloss:1.03979\teval-mlogloss:1.0947\n",
      "[98]\ttrain-mlogloss:1.03906\teval-mlogloss:1.09443\n",
      "[99]\ttrain-mlogloss:1.03843\teval-mlogloss:1.09419\n",
      "[100]\ttrain-mlogloss:1.03761\teval-mlogloss:1.09396\n",
      "[101]\ttrain-mlogloss:1.03693\teval-mlogloss:1.09375\n",
      "[102]\ttrain-mlogloss:1.03613\teval-mlogloss:1.09354\n",
      "[103]\ttrain-mlogloss:1.03548\teval-mlogloss:1.09333\n",
      "[104]\ttrain-mlogloss:1.03469\teval-mlogloss:1.09306\n",
      "[105]\ttrain-mlogloss:1.03394\teval-mlogloss:1.09283\n",
      "[106]\ttrain-mlogloss:1.03335\teval-mlogloss:1.09269\n",
      "[107]\ttrain-mlogloss:1.03279\teval-mlogloss:1.09257\n",
      "[108]\ttrain-mlogloss:1.03219\teval-mlogloss:1.09239\n",
      "[109]\ttrain-mlogloss:1.03145\teval-mlogloss:1.09217\n",
      "[110]\ttrain-mlogloss:1.0307\teval-mlogloss:1.09203\n",
      "[111]\ttrain-mlogloss:1.03013\teval-mlogloss:1.09189\n",
      "[112]\ttrain-mlogloss:1.02931\teval-mlogloss:1.09169\n",
      "[113]\ttrain-mlogloss:1.02883\teval-mlogloss:1.09157\n",
      "[114]\ttrain-mlogloss:1.02821\teval-mlogloss:1.09141\n",
      "[115]\ttrain-mlogloss:1.02758\teval-mlogloss:1.09127\n",
      "[116]\ttrain-mlogloss:1.02697\teval-mlogloss:1.09116\n",
      "[117]\ttrain-mlogloss:1.02648\teval-mlogloss:1.09107\n",
      "[118]\ttrain-mlogloss:1.02586\teval-mlogloss:1.09092\n",
      "[119]\ttrain-mlogloss:1.02513\teval-mlogloss:1.09079\n",
      "[120]\ttrain-mlogloss:1.02452\teval-mlogloss:1.0907\n",
      "[121]\ttrain-mlogloss:1.02387\teval-mlogloss:1.09063\n",
      "[122]\ttrain-mlogloss:1.02333\teval-mlogloss:1.09055\n",
      "[123]\ttrain-mlogloss:1.02252\teval-mlogloss:1.09044\n",
      "[124]\ttrain-mlogloss:1.022\teval-mlogloss:1.09031\n",
      "[125]\ttrain-mlogloss:1.02139\teval-mlogloss:1.09027\n",
      "[126]\ttrain-mlogloss:1.02087\teval-mlogloss:1.09016\n",
      "[127]\ttrain-mlogloss:1.02007\teval-mlogloss:1.09003\n",
      "[128]\ttrain-mlogloss:1.01938\teval-mlogloss:1.08994\n",
      "[129]\ttrain-mlogloss:1.01875\teval-mlogloss:1.08987\n",
      "[130]\ttrain-mlogloss:1.01813\teval-mlogloss:1.0898\n",
      "[131]\ttrain-mlogloss:1.01752\teval-mlogloss:1.08969\n",
      "[132]\ttrain-mlogloss:1.01689\teval-mlogloss:1.08956\n",
      "[133]\ttrain-mlogloss:1.01626\teval-mlogloss:1.08947\n",
      "[134]\ttrain-mlogloss:1.01562\teval-mlogloss:1.08939\n",
      "[135]\ttrain-mlogloss:1.01486\teval-mlogloss:1.08932\n",
      "[136]\ttrain-mlogloss:1.01433\teval-mlogloss:1.08924\n",
      "[137]\ttrain-mlogloss:1.01373\teval-mlogloss:1.08916\n",
      "[138]\ttrain-mlogloss:1.01312\teval-mlogloss:1.08905\n",
      "[139]\ttrain-mlogloss:1.01255\teval-mlogloss:1.08895\n",
      "[140]\ttrain-mlogloss:1.012\teval-mlogloss:1.08891\n",
      "[141]\ttrain-mlogloss:1.01154\teval-mlogloss:1.08881\n",
      "[142]\ttrain-mlogloss:1.01108\teval-mlogloss:1.08876\n",
      "[143]\ttrain-mlogloss:1.01056\teval-mlogloss:1.08868\n",
      "[144]\ttrain-mlogloss:1.00997\teval-mlogloss:1.08862\n",
      "[145]\ttrain-mlogloss:1.00938\teval-mlogloss:1.0886\n",
      "[146]\ttrain-mlogloss:1.00875\teval-mlogloss:1.08849\n",
      "[147]\ttrain-mlogloss:1.00819\teval-mlogloss:1.08841\n",
      "[148]\ttrain-mlogloss:1.0077\teval-mlogloss:1.08837\n",
      "[149]\ttrain-mlogloss:1.00684\teval-mlogloss:1.0883\n",
      "[150]\ttrain-mlogloss:1.00628\teval-mlogloss:1.08822\n",
      "[151]\ttrain-mlogloss:1.00569\teval-mlogloss:1.08822\n",
      "[152]\ttrain-mlogloss:1.00516\teval-mlogloss:1.08818\n",
      "[153]\ttrain-mlogloss:1.00457\teval-mlogloss:1.08812\n",
      "[154]\ttrain-mlogloss:1.00405\teval-mlogloss:1.08811\n",
      "[155]\ttrain-mlogloss:1.00371\teval-mlogloss:1.08806\n",
      "[156]\ttrain-mlogloss:1.0032\teval-mlogloss:1.08806\n",
      "[157]\ttrain-mlogloss:1.00263\teval-mlogloss:1.08808\n",
      "[158]\ttrain-mlogloss:1.00199\teval-mlogloss:1.08805\n",
      "[159]\ttrain-mlogloss:1.00144\teval-mlogloss:1.08803\n",
      "[160]\ttrain-mlogloss:1.00095\teval-mlogloss:1.08797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161]\ttrain-mlogloss:1.00024\teval-mlogloss:1.08796\n",
      "[162]\ttrain-mlogloss:0.999688\teval-mlogloss:1.08793\n",
      "[163]\ttrain-mlogloss:0.999086\teval-mlogloss:1.08793\n",
      "[164]\ttrain-mlogloss:0.998575\teval-mlogloss:1.08793\n",
      "[165]\ttrain-mlogloss:0.998012\teval-mlogloss:1.08789\n",
      "[166]\ttrain-mlogloss:0.997261\teval-mlogloss:1.08785\n",
      "[167]\ttrain-mlogloss:0.996696\teval-mlogloss:1.08781\n",
      "[168]\ttrain-mlogloss:0.996144\teval-mlogloss:1.08783\n",
      "[169]\ttrain-mlogloss:0.995657\teval-mlogloss:1.08777\n",
      "[170]\ttrain-mlogloss:0.995063\teval-mlogloss:1.08777\n",
      "[171]\ttrain-mlogloss:0.994647\teval-mlogloss:1.08778\n",
      "[172]\ttrain-mlogloss:0.994214\teval-mlogloss:1.08774\n",
      "[173]\ttrain-mlogloss:0.993711\teval-mlogloss:1.08774\n",
      "[174]\ttrain-mlogloss:0.993065\teval-mlogloss:1.08771\n",
      "[175]\ttrain-mlogloss:0.992392\teval-mlogloss:1.08773\n",
      "[176]\ttrain-mlogloss:0.991855\teval-mlogloss:1.08775\n",
      "[177]\ttrain-mlogloss:0.991375\teval-mlogloss:1.08773\n",
      "[178]\ttrain-mlogloss:0.990732\teval-mlogloss:1.0877\n",
      "[179]\ttrain-mlogloss:0.99022\teval-mlogloss:1.08766\n",
      "[180]\ttrain-mlogloss:0.989675\teval-mlogloss:1.08764\n",
      "[181]\ttrain-mlogloss:0.989251\teval-mlogloss:1.08759\n",
      "[182]\ttrain-mlogloss:0.988723\teval-mlogloss:1.08758\n",
      "[183]\ttrain-mlogloss:0.988417\teval-mlogloss:1.08758\n",
      "[184]\ttrain-mlogloss:0.987917\teval-mlogloss:1.08758\n",
      "[185]\ttrain-mlogloss:0.987334\teval-mlogloss:1.08757\n",
      "[186]\ttrain-mlogloss:0.986883\teval-mlogloss:1.08755\n",
      "[187]\ttrain-mlogloss:0.986344\teval-mlogloss:1.08761\n",
      "[188]\ttrain-mlogloss:0.98573\teval-mlogloss:1.08759\n",
      "[189]\ttrain-mlogloss:0.985262\teval-mlogloss:1.0876\n",
      "[190]\ttrain-mlogloss:0.984732\teval-mlogloss:1.0876\n",
      "[191]\ttrain-mlogloss:0.984139\teval-mlogloss:1.08762\n",
      "[192]\ttrain-mlogloss:0.983567\teval-mlogloss:1.08759\n",
      "[193]\ttrain-mlogloss:0.982985\teval-mlogloss:1.0876\n",
      "[194]\ttrain-mlogloss:0.982428\teval-mlogloss:1.0876\n",
      "[195]\ttrain-mlogloss:0.981837\teval-mlogloss:1.08761\n",
      "[196]\ttrain-mlogloss:0.981238\teval-mlogloss:1.08759\n",
      "[197]\ttrain-mlogloss:0.980837\teval-mlogloss:1.08761\n",
      "[198]\ttrain-mlogloss:0.980256\teval-mlogloss:1.08762\n",
      "[199]\ttrain-mlogloss:0.97974\teval-mlogloss:1.08761\n",
      "[200]\ttrain-mlogloss:0.979214\teval-mlogloss:1.08756\n",
      "[201]\ttrain-mlogloss:0.978694\teval-mlogloss:1.08764\n",
      "[202]\ttrain-mlogloss:0.978324\teval-mlogloss:1.08766\n",
      "[203]\ttrain-mlogloss:0.977929\teval-mlogloss:1.08764\n",
      "[204]\ttrain-mlogloss:0.977405\teval-mlogloss:1.08762\n",
      "[205]\ttrain-mlogloss:0.976727\teval-mlogloss:1.08758\n",
      "[206]\ttrain-mlogloss:0.976223\teval-mlogloss:1.08756\n",
      "Stopping. Best iteration:\n",
      "[186]\ttrain-mlogloss:0.986883\teval-mlogloss:1.08755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "watch_list = [(dtrn, 'train'), (dvld, 'eval')]\n",
    "model = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open(\"model/xgb.baseline.pkl\", \"wb\"))\n",
    "best_ntree_limit = model.best_ntree_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vld = trn[trn['fecha_dato'] == vld_date]\n",
    "ncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n",
    "for prod in prods:\n",
    "    prev = prod + '_prev'\n",
    "    padd = prod + '_add'\n",
    "    vld[padd] = vld[prod] - vld[prev]\n",
    "    \n",
    "add_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\n",
    "add_vld_list = [list() for i in range(len(ncodpers_vld))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_vld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vld = 0\n",
    "for ncodper in range(len(ncodpers_vld)):\n",
    "    for prod in range(len(prods)):\n",
    "        if add_vld[ncodper, prod] > 0:\n",
    "            add_vld_list[ncodper].append(prod)\n",
    "            count_vld += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04266379915553903\n"
     ]
    }
   ],
   "source": [
    "print(mapk(add_vld_list, add_vld_list, 7, 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_vld = vld.as_matrix(columns=features)\n",
    "Y_vld = vld.as_matrix(columns=['y'])\n",
    "dvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n",
    "preds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "preds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_vld = []\n",
    "for ncodper, pred in zip(ncodpers_vld, preds_vld):\n",
    "    y_prods = [(y, p, ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    result_vld.append([ip for y,p,ip in y_prods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03645581690898608\n"
     ]
    }
   ],
   "source": [
    "print(mapk(add_vld_list, result_vld, 7, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Total Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_all = XY.as_matrix(columns=features)\n",
    "Y_all = XY.as_matrix(columns=['y'])\n",
    "dall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\n",
    "watch_list = [(dall, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.74834\n",
      "[1]\ttrain-mlogloss:2.51448\n",
      "[2]\ttrain-mlogloss:2.32418\n",
      "[3]\ttrain-mlogloss:2.17134\n",
      "[4]\ttrain-mlogloss:2.05217\n",
      "[5]\ttrain-mlogloss:1.95331\n",
      "[6]\ttrain-mlogloss:1.86703\n",
      "[7]\ttrain-mlogloss:1.79362\n",
      "[8]\ttrain-mlogloss:1.7297\n",
      "[9]\ttrain-mlogloss:1.67507\n",
      "[10]\ttrain-mlogloss:1.62983\n",
      "[11]\ttrain-mlogloss:1.58495\n",
      "[12]\ttrain-mlogloss:1.54624\n",
      "[13]\ttrain-mlogloss:1.51151\n",
      "[14]\ttrain-mlogloss:1.47872\n",
      "[15]\ttrain-mlogloss:1.44883\n",
      "[16]\ttrain-mlogloss:1.4223\n",
      "[17]\ttrain-mlogloss:1.39709\n",
      "[18]\ttrain-mlogloss:1.37508\n",
      "[19]\ttrain-mlogloss:1.35382\n",
      "[20]\ttrain-mlogloss:1.335\n",
      "[21]\ttrain-mlogloss:1.31823\n",
      "[22]\ttrain-mlogloss:1.30168\n",
      "[23]\ttrain-mlogloss:1.28713\n",
      "[24]\ttrain-mlogloss:1.27323\n",
      "[25]\ttrain-mlogloss:1.26112\n",
      "[26]\ttrain-mlogloss:1.24877\n",
      "[27]\ttrain-mlogloss:1.23767\n",
      "[28]\ttrain-mlogloss:1.22734\n",
      "[29]\ttrain-mlogloss:1.21743\n",
      "[30]\ttrain-mlogloss:1.20832\n",
      "[31]\ttrain-mlogloss:1.19983\n",
      "[32]\ttrain-mlogloss:1.19221\n",
      "[33]\ttrain-mlogloss:1.18441\n",
      "[34]\ttrain-mlogloss:1.17759\n",
      "[35]\ttrain-mlogloss:1.17078\n",
      "[36]\ttrain-mlogloss:1.16477\n",
      "[37]\ttrain-mlogloss:1.15902\n",
      "[38]\ttrain-mlogloss:1.15328\n",
      "[39]\ttrain-mlogloss:1.14797\n",
      "[40]\ttrain-mlogloss:1.14292\n",
      "[41]\ttrain-mlogloss:1.13836\n",
      "[42]\ttrain-mlogloss:1.1339\n",
      "[43]\ttrain-mlogloss:1.12969\n",
      "[44]\ttrain-mlogloss:1.12576\n",
      "[45]\ttrain-mlogloss:1.12181\n",
      "[46]\ttrain-mlogloss:1.11828\n",
      "[47]\ttrain-mlogloss:1.11468\n",
      "[48]\ttrain-mlogloss:1.11159\n",
      "[49]\ttrain-mlogloss:1.10853\n",
      "[50]\ttrain-mlogloss:1.10548\n",
      "[51]\ttrain-mlogloss:1.10274\n",
      "[52]\ttrain-mlogloss:1.1\n",
      "[53]\ttrain-mlogloss:1.09733\n",
      "[54]\ttrain-mlogloss:1.09495\n",
      "[55]\ttrain-mlogloss:1.09252\n",
      "[56]\ttrain-mlogloss:1.09019\n",
      "[57]\ttrain-mlogloss:1.08805\n",
      "[58]\ttrain-mlogloss:1.08601\n",
      "[59]\ttrain-mlogloss:1.08405\n",
      "[60]\ttrain-mlogloss:1.08215\n",
      "[61]\ttrain-mlogloss:1.08035\n",
      "[62]\ttrain-mlogloss:1.07882\n",
      "[63]\ttrain-mlogloss:1.07709\n",
      "[64]\ttrain-mlogloss:1.07566\n",
      "[65]\ttrain-mlogloss:1.0742\n",
      "[66]\ttrain-mlogloss:1.07265\n",
      "[67]\ttrain-mlogloss:1.07117\n",
      "[68]\ttrain-mlogloss:1.06978\n",
      "[69]\ttrain-mlogloss:1.06841\n",
      "[70]\ttrain-mlogloss:1.06712\n",
      "[71]\ttrain-mlogloss:1.06594\n",
      "[72]\ttrain-mlogloss:1.06472\n",
      "[73]\ttrain-mlogloss:1.06358\n",
      "[74]\ttrain-mlogloss:1.06255\n",
      "[75]\ttrain-mlogloss:1.06142\n",
      "[76]\ttrain-mlogloss:1.06045\n",
      "[77]\ttrain-mlogloss:1.0594\n",
      "[78]\ttrain-mlogloss:1.05835\n",
      "[79]\ttrain-mlogloss:1.0573\n",
      "[80]\ttrain-mlogloss:1.05635\n",
      "[81]\ttrain-mlogloss:1.05547\n",
      "[82]\ttrain-mlogloss:1.05463\n",
      "[83]\ttrain-mlogloss:1.05363\n",
      "[84]\ttrain-mlogloss:1.05274\n",
      "[85]\ttrain-mlogloss:1.05197\n",
      "[86]\ttrain-mlogloss:1.05125\n",
      "[87]\ttrain-mlogloss:1.05033\n",
      "[88]\ttrain-mlogloss:1.04955\n",
      "[89]\ttrain-mlogloss:1.04881\n",
      "[90]\ttrain-mlogloss:1.04812\n",
      "[91]\ttrain-mlogloss:1.04728\n",
      "[92]\ttrain-mlogloss:1.04653\n",
      "[93]\ttrain-mlogloss:1.04582\n",
      "[94]\ttrain-mlogloss:1.04519\n",
      "[95]\ttrain-mlogloss:1.04455\n",
      "[96]\ttrain-mlogloss:1.04389\n",
      "[97]\ttrain-mlogloss:1.04326\n",
      "[98]\ttrain-mlogloss:1.04253\n",
      "[99]\ttrain-mlogloss:1.04195\n",
      "[100]\ttrain-mlogloss:1.04116\n",
      "[101]\ttrain-mlogloss:1.04061\n",
      "[102]\ttrain-mlogloss:1.03998\n",
      "[103]\ttrain-mlogloss:1.0394\n",
      "[104]\ttrain-mlogloss:1.03877\n",
      "[105]\ttrain-mlogloss:1.03816\n",
      "[106]\ttrain-mlogloss:1.0375\n",
      "[107]\ttrain-mlogloss:1.0369\n",
      "[108]\ttrain-mlogloss:1.03635\n",
      "[109]\ttrain-mlogloss:1.03588\n",
      "[110]\ttrain-mlogloss:1.03524\n",
      "[111]\ttrain-mlogloss:1.03463\n",
      "[112]\ttrain-mlogloss:1.03403\n",
      "[113]\ttrain-mlogloss:1.03351\n",
      "[114]\ttrain-mlogloss:1.03294\n",
      "[115]\ttrain-mlogloss:1.03247\n",
      "[116]\ttrain-mlogloss:1.03181\n",
      "[117]\ttrain-mlogloss:1.03131\n",
      "[118]\ttrain-mlogloss:1.03071\n",
      "[119]\ttrain-mlogloss:1.03012\n",
      "[120]\ttrain-mlogloss:1.0296\n",
      "[121]\ttrain-mlogloss:1.02918\n",
      "[122]\ttrain-mlogloss:1.02851\n",
      "[123]\ttrain-mlogloss:1.02804\n",
      "[124]\ttrain-mlogloss:1.02757\n",
      "[125]\ttrain-mlogloss:1.02713\n",
      "[126]\ttrain-mlogloss:1.02672\n",
      "[127]\ttrain-mlogloss:1.02624\n",
      "[128]\ttrain-mlogloss:1.02576\n",
      "[129]\ttrain-mlogloss:1.02521\n",
      "[130]\ttrain-mlogloss:1.02475\n",
      "[131]\ttrain-mlogloss:1.02429\n",
      "[132]\ttrain-mlogloss:1.02372\n",
      "[133]\ttrain-mlogloss:1.02319\n",
      "[134]\ttrain-mlogloss:1.0227\n",
      "[135]\ttrain-mlogloss:1.02233\n",
      "[136]\ttrain-mlogloss:1.02189\n",
      "[137]\ttrain-mlogloss:1.02133\n",
      "[138]\ttrain-mlogloss:1.02088\n",
      "[139]\ttrain-mlogloss:1.02042\n",
      "[140]\ttrain-mlogloss:1.01988\n",
      "[141]\ttrain-mlogloss:1.01934\n",
      "[142]\ttrain-mlogloss:1.0189\n",
      "[143]\ttrain-mlogloss:1.01839\n",
      "[144]\ttrain-mlogloss:1.01782\n",
      "[145]\ttrain-mlogloss:1.01724\n",
      "[146]\ttrain-mlogloss:1.01669\n",
      "[147]\ttrain-mlogloss:1.01625\n",
      "[148]\ttrain-mlogloss:1.0158\n",
      "[149]\ttrain-mlogloss:1.01517\n",
      "[150]\ttrain-mlogloss:1.01463\n",
      "[151]\ttrain-mlogloss:1.01412\n",
      "[152]\ttrain-mlogloss:1.01365\n",
      "[153]\ttrain-mlogloss:1.0132\n",
      "[154]\ttrain-mlogloss:1.01274\n",
      "[155]\ttrain-mlogloss:1.01228\n",
      "[156]\ttrain-mlogloss:1.01172\n",
      "[157]\ttrain-mlogloss:1.01117\n",
      "[158]\ttrain-mlogloss:1.01074\n",
      "[159]\ttrain-mlogloss:1.01027\n",
      "[160]\ttrain-mlogloss:1.00974\n",
      "[161]\ttrain-mlogloss:1.0092\n",
      "[162]\ttrain-mlogloss:1.00877\n",
      "[163]\ttrain-mlogloss:1.00823\n",
      "[164]\ttrain-mlogloss:1.00779\n",
      "[165]\ttrain-mlogloss:1.00736\n",
      "[166]\ttrain-mlogloss:1.00684\n",
      "[167]\ttrain-mlogloss:1.0063\n",
      "[168]\ttrain-mlogloss:1.00585\n",
      "[169]\ttrain-mlogloss:1.00539\n",
      "[170]\ttrain-mlogloss:1.00484\n",
      "[171]\ttrain-mlogloss:1.00447\n",
      "[172]\ttrain-mlogloss:1.00395\n",
      "[173]\ttrain-mlogloss:1.0033\n",
      "[174]\ttrain-mlogloss:1.00285\n",
      "[175]\ttrain-mlogloss:1.00246\n",
      "[176]\ttrain-mlogloss:1.00191\n",
      "[177]\ttrain-mlogloss:1.00151\n",
      "[178]\ttrain-mlogloss:1.00102\n",
      "[179]\ttrain-mlogloss:1.00036\n",
      "[180]\ttrain-mlogloss:0.999986\n",
      "[181]\ttrain-mlogloss:0.999459\n",
      "[182]\ttrain-mlogloss:0.999059\n",
      "[183]\ttrain-mlogloss:0.998613\n",
      "[184]\ttrain-mlogloss:0.998169\n",
      "[185]\ttrain-mlogloss:0.997564\n",
      "[186]\ttrain-mlogloss:0.997076\n",
      "[187]\ttrain-mlogloss:0.996585\n",
      "[188]\ttrain-mlogloss:0.996003\n",
      "[189]\ttrain-mlogloss:0.995623\n",
      "[190]\ttrain-mlogloss:0.995169\n",
      "[191]\ttrain-mlogloss:0.99467\n",
      "[192]\ttrain-mlogloss:0.994291\n",
      "[193]\ttrain-mlogloss:0.993934\n",
      "[194]\ttrain-mlogloss:0.993297\n",
      "[195]\ttrain-mlogloss:0.992841\n",
      "[196]\ttrain-mlogloss:0.992318\n",
      "[197]\ttrain-mlogloss:0.991868\n",
      "[198]\ttrain-mlogloss:0.991434\n",
      "[199]\ttrain-mlogloss:0.99093\n",
      "[200]\ttrain-mlogloss:0.990409\n",
      "[201]\ttrain-mlogloss:0.989871\n",
      "[202]\ttrain-mlogloss:0.989322\n",
      "[203]\ttrain-mlogloss:0.988913\n",
      "[204]\ttrain-mlogloss:0.98848\n",
      "[205]\ttrain-mlogloss:0.988045\n",
      "[206]\ttrain-mlogloss:0.987613\n",
      "[207]\ttrain-mlogloss:0.987201\n",
      "[208]\ttrain-mlogloss:0.98666\n",
      "[209]\ttrain-mlogloss:0.986239\n",
      "[210]\ttrain-mlogloss:0.985692\n",
      "[211]\ttrain-mlogloss:0.985209\n",
      "[212]\ttrain-mlogloss:0.984807\n",
      "[213]\ttrain-mlogloss:0.984259\n",
      "[214]\ttrain-mlogloss:0.983844\n",
      "[215]\ttrain-mlogloss:0.983406\n",
      "[216]\ttrain-mlogloss:0.982852\n",
      "[217]\ttrain-mlogloss:0.982435\n",
      "[218]\ttrain-mlogloss:0.981962\n",
      "[219]\ttrain-mlogloss:0.981529\n",
      "[220]\ttrain-mlogloss:0.981039\n",
      "[221]\ttrain-mlogloss:0.980638\n",
      "[222]\ttrain-mlogloss:0.980126\n",
      "[223]\ttrain-mlogloss:0.97955\n",
      "[224]\ttrain-mlogloss:0.979106\n",
      "[225]\ttrain-mlogloss:0.978553\n",
      "[226]\ttrain-mlogloss:0.978031\n",
      "[227]\ttrain-mlogloss:0.977662\n",
      "[228]\ttrain-mlogloss:0.97717\n",
      "[229]\ttrain-mlogloss:0.976677\n"
     ]
    }
   ],
   "source": [
    "best_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n",
    "model = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      "('renta', 19253)\n",
      "('age', 18196)\n",
      "('antiguedad', 17060)\n",
      "('age_prev', 12423)\n",
      "('antiguedad_prev', 11739)\n",
      "('fecha_alta_month', 11090)\n",
      "('nomprov', 10902)\n",
      "('fecha_alta_year', 8821)\n",
      "('renta_prev', 7626)\n",
      "('canal_entrada', 7387)\n",
      "('nomprov_prev', 6004)\n",
      "('canal_entrada_prev', 4385)\n",
      "('fecha_alta_month_prev', 4123)\n",
      "('ind_recibo_ult1_prev', 3188)\n",
      "('sexo', 3178)\n",
      "('fecha_alta_year_prev', 3097)\n",
      "('ind_ecue_fin_ult1_prev', 2972)\n",
      "('ind_cco_fin_ult1_prev', 2912)\n",
      "('ind_cno_fin_ult1_prev', 2692)\n",
      "('segmento', 2192)\n",
      "('ind_tjcr_fin_ult1_prev', 2043)\n",
      "('ind_reca_fin_ult1_prev', 1994)\n",
      "('segmento_prev', 1892)\n",
      "('tiprel_1mes', 1781)\n",
      "('ind_nom_pens_ult1_prev', 1617)\n",
      "('ind_nomina_ult1_prev', 1506)\n",
      "('ind_valo_fin_ult1_prev', 1482)\n",
      "('ind_dela_fin_ult1_prev', 1447)\n",
      "('ind_ctop_fin_ult1_prev', 1337)\n",
      "('ind_actividad_cliente', 1286)\n",
      "('tiprel_1mes_prev', 1240)\n",
      "('sexo_prev', 1189)\n",
      "('ind_ctpp_fin_ult1_prev', 1124)\n",
      "('ind_fond_fin_ult1_prev', 968)\n",
      "('ind_ctma_fin_ult1_prev', 865)\n",
      "('ind_actividad_cliente_prev', 862)\n",
      "('indext', 765)\n",
      "('ind_nuevo', 677)\n",
      "('ind_plan_fin_ult1_prev', 607)\n",
      "('ind_hip_fin_ult1_prev', 513)\n",
      "('ind_nuevo_prev', 403)\n",
      "('indext_prev', 372)\n",
      "('ind_deco_fin_ult1_prev', 357)\n",
      "('pais_residencia', 217)\n",
      "('ind_empleado_prev', 169)\n",
      "('ind_viv_fin_ult1_prev', 163)\n",
      "('indrel', 162)\n",
      "('ind_empleado', 154)\n",
      "('pais_residencia_prev', 132)\n",
      "('ind_deme_fin_ult1_prev', 101)\n",
      "('ind_ctju_fin_ult1_prev', 84)\n",
      "('ind_pres_fin_ult1_prev', 75)\n",
      "('ult_fec_cli_1t_month', 48)\n",
      "('indfall', 36)\n",
      "('ind_cder_fin_ult1_prev', 34)\n",
      "('conyuemp_prev', 24)\n",
      "('indfall_prev', 23)\n",
      "('ult_fec_cli_1t_year', 17)\n",
      "('indresi_prev', 11)\n",
      "('indresi', 10)\n",
      "('conyuemp', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature importance:\")\n",
    "for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n",
    "    print(kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_tst = tst.as_matrix(columns=features)\n",
    "dtst = xgb.DMatrix(X_tst, feature_names=features)\n",
    "preds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "E:\\ProgramData\\Anaconda3\\envs\\dsc\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ncodpers_tst = tst.as_matrix(columns=['ncodpers'])\n",
    "preds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file = open('model/xgb.baseline.2015-06-28(2)', 'w')\n",
    "submit_file.write('ncodpers,added_products\\n')\n",
    "for ncodper, pred in zip(ncodpers_tst, preds_tst):\n",
    "    y_prods = [(y,p,ip) for y, p, ip in zip(pred, prods, range(len(prods)))]\n",
    "    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n",
    "    y_prods = [p for y,p,ip in y_prods]\n",
    "    submit_file.write('{}, {}\\n'.format(int(ncodper), ''.join(y_prods)))\n",
    "submit_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
